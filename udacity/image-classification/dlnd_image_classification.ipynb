{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 9:\n",
      "Image - Min Value: 2 Max Value: 246\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQQ8AjQl6AGhM0ANAY4IeABr7X/7z++hfsx+YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbf45d5c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 9\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    \n",
    "    # 殊途同归\n",
    "    #return  a +  (x - grayscale_min + b-1) / (grayscale_max - grayscale_min)\n",
    "    #return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    return (x.astype(float))/255\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #onehots = []\n",
    "    #for each in x :\n",
    "    #    zero= np.zeros(10)\n",
    "    #    zero[each] =1\n",
    "    #    onehots.append(zero)\n",
    "    # return np.array(onehots)\n",
    "    \n",
    "    #更好的方法，    \n",
    "    return np.eye(10)[x]\n",
    "    #最好的方法\n",
    "    #from sklearn.preprocessing import LabelBinarizer\n",
    "    #encoder = LabelBinarizer().fit(range(10))\n",
    "    #encoder.transform(x)\n",
    "\n",
    "    #但是如果x列表并非规则的0-9，而是 11 -19 这种，此方法就不行。\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # return  tf.placeholder(tf.float32, shape=[None, image_shape[0],  image_shape[1],  image_shape[2]], name='x')\n",
    "    # 你可以考虑 unpacking (https://docs.python.org/2/tutorial/controlflow.html#unpacking-argument-lists)一个列表 :\n",
    "    return  tf.placeholder(tf.float32, shape=[None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape=[None,n_classes],name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 5)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # 卷积\n",
    "        #设置卷积核参数\n",
    "    print(x_tensor.get_shape())\n",
    "    FilterW= tf.Variable(tf.truncated_normal((conv_ksize[0], conv_ksize[1], x_tensor.get_shape()[3].value, conv_num_outputs),stddev=0.05)) ## (height, width, input_depth, output_depth)\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    bias=tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv = tf.nn.conv2d(x_tensor, FilterW, strides, padding)\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv= tf.nn.relu(conv)\n",
    "    #池化\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    return tf.nn.max_pool(conv, ksize, strides, padding)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "    #return np.prod(x_tensor.shape.as_list()[1:])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fc1 =tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    return fc1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.01))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 8, 8, 32)\n",
      "(?, 2, 2, 64)\n",
      "(?, 32, 32, 3)\n",
      "(?, 8, 8, 32)\n",
      "(?, 2, 2, 64)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    # 卷积层的本质实际上是一个个可训练滤波器的集合，执行的功能是对图像进行采样。因而一些比较基础的设计卷积层的思想包括：\n",
    "    # 各卷积层之间，卷积核的尺度应逐步增大，例如 2 - 4 - 6 这样的形式（当然也有一直使用小核的情况）\n",
    "    # 卷积的输的通道数出也类似如上的表述，可以是 32 - 64 - 128 这样的形式\n",
    "    # Stride 越小，那么图像保留的细节就越多；不过也有可能会带来过多的细节，不利于训练，所以需要权衡。一般取 1～3 之类的数值\n",
    "    # Pooling 层的逻辑大致相同，不过一般我们取比较小的 pooling 核尺寸，避免特征被过度的舍弃。\n",
    "    conv1 = conv2d_maxpool(x, 32, [2,2], [2,2], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 64, [4,4], [2,2], [2,2], [2,2])\n",
    "    conv3 = conv2d_maxpool(conv2, 128, [6,6], [2,2], [2,2], [2,2])\n",
    "    \n",
    "    \n",
    "\n",
    "    # TODApply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv3)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    # 个人的观点是，此处全连接层的作用就是一个分类器，即把卷积层提取的特征进行分类。\n",
    "    #那么这边层数的设定应当根据你 flatten 层得到的 tensor 的规模来，然后逐步的缩小，你可以考虑类似 512 - 256 -10 的设定。\n",
    "    #参考资料\n",
    "    # How to choose the number of hidden layers and nodes in a feedforward neural network?\n",
    "    # Why not set dropout in cnn layers\n",
    "    fl1 = fully_conn(fl0, 512)\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 256)\n",
    "    fl2= tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    out = output(fl2,10)\n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:valid_features, y: valid_labels, keep_prob:1.0})\n",
    "    print('Loss: {:>10.4f}  Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2235  Validation Accuracy: 0.199600\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2266  Validation Accuracy: 0.216000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1031  Validation Accuracy: 0.272800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9608  Validation Accuracy: 0.311000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.8784  Validation Accuracy: 0.341200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.8158  Validation Accuracy: 0.377800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.7600  Validation Accuracy: 0.391000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.6365  Validation Accuracy: 0.407400\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.5446  Validation Accuracy: 0.411000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.5357  Validation Accuracy: 0.418400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.3943  Validation Accuracy: 0.424000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.3427  Validation Accuracy: 0.425800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.2415  Validation Accuracy: 0.442000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.1788  Validation Accuracy: 0.448600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.0736  Validation Accuracy: 0.466400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.9978  Validation Accuracy: 0.470600\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.9309  Validation Accuracy: 0.476800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.8492  Validation Accuracy: 0.483600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.7570  Validation Accuracy: 0.492800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.7191  Validation Accuracy: 0.491600\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.6401  Validation Accuracy: 0.501600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.5725  Validation Accuracy: 0.501400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.5576  Validation Accuracy: 0.504800\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.5301  Validation Accuracy: 0.509000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.5121  Validation Accuracy: 0.512200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.4859  Validation Accuracy: 0.500000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.4960  Validation Accuracy: 0.502400\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.5004  Validation Accuracy: 0.479600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.4276  Validation Accuracy: 0.519800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.4670  Validation Accuracy: 0.504200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.4029  Validation Accuracy: 0.513800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.3728  Validation Accuracy: 0.514400\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.3550  Validation Accuracy: 0.512600\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.5163  Validation Accuracy: 0.475200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.3491  Validation Accuracy: 0.509400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.3175  Validation Accuracy: 0.508400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.2683  Validation Accuracy: 0.514400\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2619  Validation Accuracy: 0.511600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.2511  Validation Accuracy: 0.509400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.3145  Validation Accuracy: 0.473800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.2501  Validation Accuracy: 0.503200\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.2514  Validation Accuracy: 0.515800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.3507  Validation Accuracy: 0.461600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.2719  Validation Accuracy: 0.487800\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.2225  Validation Accuracy: 0.516200\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.2310  Validation Accuracy: 0.510200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.2378  Validation Accuracy: 0.511200\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.2433  Validation Accuracy: 0.486200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.2275  Validation Accuracy: 0.485800\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.1956  Validation Accuracy: 0.500400\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.2024  Validation Accuracy: 0.512200\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.2378  Validation Accuracy: 0.484600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.2430  Validation Accuracy: 0.498000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1647  Validation Accuracy: 0.509600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.1344  Validation Accuracy: 0.500200\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.1509  Validation Accuracy: 0.477400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.1777  Validation Accuracy: 0.459400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.2014  Validation Accuracy: 0.462200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.1467  Validation Accuracy: 0.520800\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.1083  Validation Accuracy: 0.516200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.1102  Validation Accuracy: 0.502800\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.1399  Validation Accuracy: 0.482600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1418  Validation Accuracy: 0.459200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1179  Validation Accuracy: 0.490200\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.1151  Validation Accuracy: 0.510800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.0763  Validation Accuracy: 0.506200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.0795  Validation Accuracy: 0.481800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0772  Validation Accuracy: 0.488200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0817  Validation Accuracy: 0.493800\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0663  Validation Accuracy: 0.504200\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0593  Validation Accuracy: 0.500200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0547  Validation Accuracy: 0.510800\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0545  Validation Accuracy: 0.502200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0619  Validation Accuracy: 0.493000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0773  Validation Accuracy: 0.495000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.1275  Validation Accuracy: 0.493600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.1382  Validation Accuracy: 0.492600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0997  Validation Accuracy: 0.495800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0486  Validation Accuracy: 0.515600\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0446  Validation Accuracy: 0.506800\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0492  Validation Accuracy: 0.510400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0539  Validation Accuracy: 0.498200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0479  Validation Accuracy: 0.515200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0538  Validation Accuracy: 0.493000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0479  Validation Accuracy: 0.491400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0531  Validation Accuracy: 0.484600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0588  Validation Accuracy: 0.479400\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0938  Validation Accuracy: 0.444600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.1850  Validation Accuracy: 0.443400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.1029  Validation Accuracy: 0.482400\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0677  Validation Accuracy: 0.493800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0473  Validation Accuracy: 0.499800\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0444  Validation Accuracy: 0.501400\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0457  Validation Accuracy: 0.506400\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0739  Validation Accuracy: 0.495000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0399  Validation Accuracy: 0.491400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0322  Validation Accuracy: 0.486200\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0331  Validation Accuracy: 0.487000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.0309  Validation Accuracy: 0.482200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0364  Validation Accuracy: 0.477800\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2366  Validation Accuracy: 0.188600\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.0798  Validation Accuracy: 0.245800\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.8074  Validation Accuracy: 0.278800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.7452  Validation Accuracy: 0.317200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.7472  Validation Accuracy: 0.329000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0059  Validation Accuracy: 0.374600\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     1.6463  Validation Accuracy: 0.397800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.3563  Validation Accuracy: 0.393400\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.5764  Validation Accuracy: 0.409600\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.5501  Validation Accuracy: 0.421000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.6852  Validation Accuracy: 0.435600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.4809  Validation Accuracy: 0.445200\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.1698  Validation Accuracy: 0.455400\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.4508  Validation Accuracy: 0.465600\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.4477  Validation Accuracy: 0.480800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.4524  Validation Accuracy: 0.491800\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.2996  Validation Accuracy: 0.465600\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.9946  Validation Accuracy: 0.475400\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.3040  Validation Accuracy: 0.502800\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.3346  Validation Accuracy: 0.491000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.3093  Validation Accuracy: 0.525800\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.1451  Validation Accuracy: 0.500200\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.8449  Validation Accuracy: 0.501200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.1842  Validation Accuracy: 0.523200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.1579  Validation Accuracy: 0.533400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.2332  Validation Accuracy: 0.536600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.0385  Validation Accuracy: 0.537600\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.7636  Validation Accuracy: 0.515800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.0356  Validation Accuracy: 0.533400\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.0060  Validation Accuracy: 0.536800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.0848  Validation Accuracy: 0.551200\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.9520  Validation Accuracy: 0.541200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.6739  Validation Accuracy: 0.535800\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.9339  Validation Accuracy: 0.552800\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.8797  Validation Accuracy: 0.551400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.9330  Validation Accuracy: 0.569400\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.8633  Validation Accuracy: 0.545600\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.6521  Validation Accuracy: 0.535000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.8300  Validation Accuracy: 0.561800\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.7834  Validation Accuracy: 0.575000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.8584  Validation Accuracy: 0.579800\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.7730  Validation Accuracy: 0.558000\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.5546  Validation Accuracy: 0.571600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.7460  Validation Accuracy: 0.566200\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.7281  Validation Accuracy: 0.581000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.8188  Validation Accuracy: 0.584800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.7299  Validation Accuracy: 0.568600\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.5138  Validation Accuracy: 0.575000\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.6709  Validation Accuracy: 0.586000\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.6366  Validation Accuracy: 0.580800\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     0.7037  Validation Accuracy: 0.594200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     0.6776  Validation Accuracy: 0.563200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     0.4867  Validation Accuracy: 0.585800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     0.6447  Validation Accuracy: 0.576600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     0.5754  Validation Accuracy: 0.588200\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     0.6339  Validation Accuracy: 0.598400\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     0.6029  Validation Accuracy: 0.591800\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     0.4643  Validation Accuracy: 0.587400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     0.5449  Validation Accuracy: 0.593000\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     0.5849  Validation Accuracy: 0.582800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     0.6223  Validation Accuracy: 0.603800\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     0.5543  Validation Accuracy: 0.596800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     0.4068  Validation Accuracy: 0.598200\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     0.5102  Validation Accuracy: 0.603800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     0.5103  Validation Accuracy: 0.598000\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     0.5829  Validation Accuracy: 0.600400\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     0.5212  Validation Accuracy: 0.598800\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     0.3734  Validation Accuracy: 0.603600\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     0.4904  Validation Accuracy: 0.608200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     0.4803  Validation Accuracy: 0.591200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     0.6033  Validation Accuracy: 0.584600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     0.5003  Validation Accuracy: 0.602200\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     0.3649  Validation Accuracy: 0.602600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     0.4825  Validation Accuracy: 0.598800\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     0.4748  Validation Accuracy: 0.600000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     0.6144  Validation Accuracy: 0.578800\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     0.4553  Validation Accuracy: 0.606800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     0.3205  Validation Accuracy: 0.613000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     0.4592  Validation Accuracy: 0.601200\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     0.4159  Validation Accuracy: 0.598400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     0.5335  Validation Accuracy: 0.602600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     0.4495  Validation Accuracy: 0.609800\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     0.2922  Validation Accuracy: 0.610400\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     0.3895  Validation Accuracy: 0.610600\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     0.3726  Validation Accuracy: 0.604800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     0.5356  Validation Accuracy: 0.606400\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     0.4354  Validation Accuracy: 0.623400\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     0.2802  Validation Accuracy: 0.605000\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     0.3811  Validation Accuracy: 0.617800\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     0.3588  Validation Accuracy: 0.607600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     0.5300  Validation Accuracy: 0.599600\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     0.4007  Validation Accuracy: 0.617400\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     0.2565  Validation Accuracy: 0.610400\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     0.3510  Validation Accuracy: 0.613600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     0.3669  Validation Accuracy: 0.603800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     0.4845  Validation Accuracy: 0.601000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     0.3940  Validation Accuracy: 0.615600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     0.2668  Validation Accuracy: 0.615600\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     0.3648  Validation Accuracy: 0.619400\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     0.3233  Validation Accuracy: 0.612000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     0.4736  Validation Accuracy: 0.608800\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     0.3687  Validation Accuracy: 0.607200\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     0.2624  Validation Accuracy: 0.602600\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     0.3389  Validation Accuracy: 0.612600\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     0.2987  Validation Accuracy: 0.614800\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     0.4915  Validation Accuracy: 0.600000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     0.3279  Validation Accuracy: 0.607800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     0.2507  Validation Accuracy: 0.611400\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     0.2871  Validation Accuracy: 0.617400\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     0.2709  Validation Accuracy: 0.610800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     0.4344  Validation Accuracy: 0.617200\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     0.3239  Validation Accuracy: 0.617000\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     0.2488  Validation Accuracy: 0.601200\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     0.3104  Validation Accuracy: 0.601200\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     0.2595  Validation Accuracy: 0.611400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     0.4261  Validation Accuracy: 0.621600\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     0.3007  Validation Accuracy: 0.616400\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     0.2297  Validation Accuracy: 0.601200\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     0.2931  Validation Accuracy: 0.601600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     0.2513  Validation Accuracy: 0.618800\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     0.3911  Validation Accuracy: 0.622200\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     0.2890  Validation Accuracy: 0.615800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     0.2333  Validation Accuracy: 0.599000\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     0.2862  Validation Accuracy: 0.599400\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     0.2511  Validation Accuracy: 0.617400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     0.3724  Validation Accuracy: 0.614400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     0.2355  Validation Accuracy: 0.614200\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     0.2303  Validation Accuracy: 0.600200\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     0.2470  Validation Accuracy: 0.599000\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     0.2304  Validation Accuracy: 0.615000\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     0.3395  Validation Accuracy: 0.614000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     0.2445  Validation Accuracy: 0.610800\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     0.2642  Validation Accuracy: 0.590800\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     0.2446  Validation Accuracy: 0.601000\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     0.2034  Validation Accuracy: 0.621000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     0.3074  Validation Accuracy: 0.616400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     0.2583  Validation Accuracy: 0.612400\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     0.2175  Validation Accuracy: 0.606200\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     0.2288  Validation Accuracy: 0.603800\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     0.2019  Validation Accuracy: 0.613600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     0.3169  Validation Accuracy: 0.615200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     0.2503  Validation Accuracy: 0.597400\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     0.1889  Validation Accuracy: 0.621400\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     0.2182  Validation Accuracy: 0.613400\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     0.1716  Validation Accuracy: 0.624400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     0.2840  Validation Accuracy: 0.616600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     0.2274  Validation Accuracy: 0.600000\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     0.1774  Validation Accuracy: 0.616000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     0.1855  Validation Accuracy: 0.609200\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     0.1705  Validation Accuracy: 0.626800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     0.2460  Validation Accuracy: 0.621000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     0.2168  Validation Accuracy: 0.598600\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     0.1729  Validation Accuracy: 0.622200\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     0.1996  Validation Accuracy: 0.608800\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     0.1795  Validation Accuracy: 0.623600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     0.2708  Validation Accuracy: 0.618600\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     0.2146  Validation Accuracy: 0.609400\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     0.1841  Validation Accuracy: 0.619200\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     0.1987  Validation Accuracy: 0.615400\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     0.1446  Validation Accuracy: 0.624800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     0.2817  Validation Accuracy: 0.616000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     0.2418  Validation Accuracy: 0.602200\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     0.1983  Validation Accuracy: 0.605200\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     0.2049  Validation Accuracy: 0.617000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     0.1575  Validation Accuracy: 0.621400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     0.3276  Validation Accuracy: 0.595400\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     0.1934  Validation Accuracy: 0.607600\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     0.1691  Validation Accuracy: 0.618600\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     0.2305  Validation Accuracy: 0.613800\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     0.1516  Validation Accuracy: 0.624600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     0.2553  Validation Accuracy: 0.606800\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     0.1950  Validation Accuracy: 0.606800\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     0.1756  Validation Accuracy: 0.620400\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     0.1707  Validation Accuracy: 0.620800\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     0.1560  Validation Accuracy: 0.617400\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     0.2328  Validation Accuracy: 0.618000\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     0.1884  Validation Accuracy: 0.610600\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     0.1917  Validation Accuracy: 0.609400\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     0.1614  Validation Accuracy: 0.624200\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     0.1228  Validation Accuracy: 0.618000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     0.2387  Validation Accuracy: 0.611800\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     0.1852  Validation Accuracy: 0.618800\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     0.1944  Validation Accuracy: 0.613600\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     0.1421  Validation Accuracy: 0.622200\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     0.1234  Validation Accuracy: 0.619800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     0.2338  Validation Accuracy: 0.624800\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     0.1592  Validation Accuracy: 0.618000\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     0.1860  Validation Accuracy: 0.614000\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     0.1380  Validation Accuracy: 0.619800\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     0.1239  Validation Accuracy: 0.620400\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     0.2172  Validation Accuracy: 0.615600\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     0.1458  Validation Accuracy: 0.611000\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     0.1892  Validation Accuracy: 0.608400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     0.1230  Validation Accuracy: 0.623600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     0.1131  Validation Accuracy: 0.621600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     0.2238  Validation Accuracy: 0.605600\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     0.1413  Validation Accuracy: 0.610200\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     0.1588  Validation Accuracy: 0.611600\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     0.1295  Validation Accuracy: 0.618800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     0.1103  Validation Accuracy: 0.614200\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     0.2157  Validation Accuracy: 0.611200\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     0.1289  Validation Accuracy: 0.606200\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     0.1477  Validation Accuracy: 0.612600\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     0.1323  Validation Accuracy: 0.616600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     0.1363  Validation Accuracy: 0.602800\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     0.2273  Validation Accuracy: 0.610400\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     0.1451  Validation Accuracy: 0.598200\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     0.1312  Validation Accuracy: 0.613200\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     0.1336  Validation Accuracy: 0.614800\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     0.1347  Validation Accuracy: 0.613800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     0.2069  Validation Accuracy: 0.614000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     0.1334  Validation Accuracy: 0.608600\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     0.1292  Validation Accuracy: 0.614000\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     0.1250  Validation Accuracy: 0.620400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     0.1093  Validation Accuracy: 0.613600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     0.1958  Validation Accuracy: 0.613400\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     0.1193  Validation Accuracy: 0.611400\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     0.1237  Validation Accuracy: 0.616200\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     0.1274  Validation Accuracy: 0.608600\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     0.1329  Validation Accuracy: 0.605400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     0.2411  Validation Accuracy: 0.594200\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     0.1309  Validation Accuracy: 0.609000\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     0.1262  Validation Accuracy: 0.611200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     0.1806  Validation Accuracy: 0.589600\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     0.1385  Validation Accuracy: 0.598400\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     0.1996  Validation Accuracy: 0.608600\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     0.1280  Validation Accuracy: 0.614000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     0.1231  Validation Accuracy: 0.603000\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     0.1095  Validation Accuracy: 0.602000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     0.1061  Validation Accuracy: 0.611800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     0.2027  Validation Accuracy: 0.605000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     0.1154  Validation Accuracy: 0.616400\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     0.1013  Validation Accuracy: 0.609000\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     0.0930  Validation Accuracy: 0.613600\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     0.0957  Validation Accuracy: 0.620000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     0.1836  Validation Accuracy: 0.613800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     0.1132  Validation Accuracy: 0.612000\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     0.1132  Validation Accuracy: 0.607400\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     0.0979  Validation Accuracy: 0.614800\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     0.0789  Validation Accuracy: 0.627200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     0.1720  Validation Accuracy: 0.618200\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     0.1343  Validation Accuracy: 0.615600\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     0.0957  Validation Accuracy: 0.618200\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     0.0945  Validation Accuracy: 0.622200\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     0.0669  Validation Accuracy: 0.618000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     0.1652  Validation Accuracy: 0.619000\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     0.1267  Validation Accuracy: 0.612600\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     0.1000  Validation Accuracy: 0.618000\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     0.0755  Validation Accuracy: 0.618400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     0.0699  Validation Accuracy: 0.628200\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     0.1494  Validation Accuracy: 0.623000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     0.0872  Validation Accuracy: 0.617200\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     0.0935  Validation Accuracy: 0.612400\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     0.0699  Validation Accuracy: 0.620600\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     0.0617  Validation Accuracy: 0.618600\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     0.1415  Validation Accuracy: 0.622200\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     0.0841  Validation Accuracy: 0.615800\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     0.1003  Validation Accuracy: 0.612800\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     0.0857  Validation Accuracy: 0.621200\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     0.0497  Validation Accuracy: 0.624800\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     0.1327  Validation Accuracy: 0.623000\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     0.0891  Validation Accuracy: 0.609000\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     0.0931  Validation Accuracy: 0.617200\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     0.0722  Validation Accuracy: 0.616800\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     0.0609  Validation Accuracy: 0.622800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     0.1291  Validation Accuracy: 0.617600\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     0.0846  Validation Accuracy: 0.621000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     0.0843  Validation Accuracy: 0.618800\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     0.0661  Validation Accuracy: 0.625400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     0.0505  Validation Accuracy: 0.625000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     0.1413  Validation Accuracy: 0.610600\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     0.0828  Validation Accuracy: 0.610200\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     0.1080  Validation Accuracy: 0.615600\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     0.0708  Validation Accuracy: 0.628200\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     0.0477  Validation Accuracy: 0.619600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     0.1331  Validation Accuracy: 0.611600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     0.0948  Validation Accuracy: 0.614800\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.1057  Validation Accuracy: 0.619200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     0.0637  Validation Accuracy: 0.623800\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     0.0407  Validation Accuracy: 0.626600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     0.1425  Validation Accuracy: 0.604400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     0.0840  Validation Accuracy: 0.621400\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     0.0906  Validation Accuracy: 0.614800\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     0.0811  Validation Accuracy: 0.617200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     0.0484  Validation Accuracy: 0.623400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     0.1302  Validation Accuracy: 0.600000\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     0.1010  Validation Accuracy: 0.606200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     0.0937  Validation Accuracy: 0.614600\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     0.0883  Validation Accuracy: 0.615600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     0.0538  Validation Accuracy: 0.616400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     0.2480  Validation Accuracy: 0.572800\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     0.1411  Validation Accuracy: 0.589200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     0.1076  Validation Accuracy: 0.605000\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     0.1249  Validation Accuracy: 0.617200\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     0.0616  Validation Accuracy: 0.613600\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     0.2283  Validation Accuracy: 0.573800\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     0.0882  Validation Accuracy: 0.599200\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.0982  Validation Accuracy: 0.606200\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.0892  Validation Accuracy: 0.622000\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     0.0509  Validation Accuracy: 0.620000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     0.1581  Validation Accuracy: 0.594000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     0.1110  Validation Accuracy: 0.589200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     0.0843  Validation Accuracy: 0.621600\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     0.0845  Validation Accuracy: 0.626600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     0.0499  Validation Accuracy: 0.620000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     0.1160  Validation Accuracy: 0.603000\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     0.1020  Validation Accuracy: 0.585600\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.0661  Validation Accuracy: 0.618800\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     0.0698  Validation Accuracy: 0.627000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     0.0532  Validation Accuracy: 0.623000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     0.1255  Validation Accuracy: 0.612600\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     0.1006  Validation Accuracy: 0.577600\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.0694  Validation Accuracy: 0.624000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.0602  Validation Accuracy: 0.621800\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     0.0409  Validation Accuracy: 0.628200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     0.1028  Validation Accuracy: 0.607400\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     0.0857  Validation Accuracy: 0.577800\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     0.0679  Validation Accuracy: 0.623000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.0533  Validation Accuracy: 0.624600\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     0.0401  Validation Accuracy: 0.621000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     0.0887  Validation Accuracy: 0.608000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     0.0726  Validation Accuracy: 0.580200\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.0696  Validation Accuracy: 0.618800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.0493  Validation Accuracy: 0.624600\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     0.0594  Validation Accuracy: 0.627000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     0.1092  Validation Accuracy: 0.601000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     0.0743  Validation Accuracy: 0.584800\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.0678  Validation Accuracy: 0.612400\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.0432  Validation Accuracy: 0.630800\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     0.0472  Validation Accuracy: 0.625000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     0.1003  Validation Accuracy: 0.602600\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     0.0634  Validation Accuracy: 0.600400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.0727  Validation Accuracy: 0.615400\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.0572  Validation Accuracy: 0.617000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     0.0597  Validation Accuracy: 0.616400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     0.0994  Validation Accuracy: 0.610400\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     0.0633  Validation Accuracy: 0.609800\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     0.0608  Validation Accuracy: 0.609000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.0391  Validation Accuracy: 0.617600\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     0.0471  Validation Accuracy: 0.615200\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     0.0875  Validation Accuracy: 0.600800\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     0.0646  Validation Accuracy: 0.610000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     0.0728  Validation Accuracy: 0.612800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.0443  Validation Accuracy: 0.620800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     0.0359  Validation Accuracy: 0.618000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     0.0725  Validation Accuracy: 0.605000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     0.0555  Validation Accuracy: 0.615200\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.0758  Validation Accuracy: 0.622200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.0665  Validation Accuracy: 0.617200\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     0.0401  Validation Accuracy: 0.620600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     0.0718  Validation Accuracy: 0.600600\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     0.0539  Validation Accuracy: 0.619600\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.0563  Validation Accuracy: 0.616800\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.0600  Validation Accuracy: 0.609200\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     0.0412  Validation Accuracy: 0.618000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     0.0790  Validation Accuracy: 0.604000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     0.0585  Validation Accuracy: 0.613000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.0673  Validation Accuracy: 0.613200\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.0765  Validation Accuracy: 0.602600\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     0.0348  Validation Accuracy: 0.617200\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     0.0826  Validation Accuracy: 0.603000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     0.0639  Validation Accuracy: 0.599400\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.0673  Validation Accuracy: 0.609800\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.0560  Validation Accuracy: 0.608800\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     0.0456  Validation Accuracy: 0.605200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     0.0920  Validation Accuracy: 0.614800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     0.0729  Validation Accuracy: 0.587000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.0731  Validation Accuracy: 0.613400\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.0685  Validation Accuracy: 0.610200\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     0.0417  Validation Accuracy: 0.600600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     0.0864  Validation Accuracy: 0.597000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     0.0724  Validation Accuracy: 0.612000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.0627  Validation Accuracy: 0.614600\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.0487  Validation Accuracy: 0.610600\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.0280  Validation Accuracy: 0.603600\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     0.0781  Validation Accuracy: 0.598600\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     0.0608  Validation Accuracy: 0.605800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.0584  Validation Accuracy: 0.610200\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.0528  Validation Accuracy: 0.612600\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     0.0298  Validation Accuracy: 0.604200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     0.0864  Validation Accuracy: 0.610400\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     0.0600  Validation Accuracy: 0.599600\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.0552  Validation Accuracy: 0.616400\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.0432  Validation Accuracy: 0.618800\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     0.0303  Validation Accuracy: 0.607200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     0.0563  Validation Accuracy: 0.611800\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     0.0451  Validation Accuracy: 0.603200\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.0499  Validation Accuracy: 0.616600\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.0523  Validation Accuracy: 0.605400\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     0.0273  Validation Accuracy: 0.601200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     0.0656  Validation Accuracy: 0.613400\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     0.0379  Validation Accuracy: 0.605200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.0426  Validation Accuracy: 0.616400\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.0468  Validation Accuracy: 0.617000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.0304  Validation Accuracy: 0.607800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     0.0558  Validation Accuracy: 0.616400\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     0.0395  Validation Accuracy: 0.606000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.0559  Validation Accuracy: 0.613400\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.0440  Validation Accuracy: 0.609200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.0295  Validation Accuracy: 0.609000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     0.0403  Validation Accuracy: 0.616600\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     0.0377  Validation Accuracy: 0.598600\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.0529  Validation Accuracy: 0.617800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.0508  Validation Accuracy: 0.616200\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     0.0464  Validation Accuracy: 0.601400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     0.0574  Validation Accuracy: 0.612800\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.0385  Validation Accuracy: 0.596600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.0413  Validation Accuracy: 0.616000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.0374  Validation Accuracy: 0.615800\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     0.0208  Validation Accuracy: 0.605000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     0.0572  Validation Accuracy: 0.607800\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.0471  Validation Accuracy: 0.595800\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.0342  Validation Accuracy: 0.614200\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.0362  Validation Accuracy: 0.614400\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     0.0279  Validation Accuracy: 0.604000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     0.0454  Validation Accuracy: 0.614600\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.0345  Validation Accuracy: 0.598200\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.0451  Validation Accuracy: 0.610200\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.0371  Validation Accuracy: 0.606600\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     0.0205  Validation Accuracy: 0.607800\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     0.0409  Validation Accuracy: 0.614600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.0421  Validation Accuracy: 0.598000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.0228  Validation Accuracy: 0.617800\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.0428  Validation Accuracy: 0.612200\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.0453  Validation Accuracy: 0.602600\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.0358  Validation Accuracy: 0.612800\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.0419  Validation Accuracy: 0.607200\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.0278  Validation Accuracy: 0.614200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.0329  Validation Accuracy: 0.607400\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.0195  Validation Accuracy: 0.606200\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.0327  Validation Accuracy: 0.603400\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     0.0465  Validation Accuracy: 0.613000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.0280  Validation Accuracy: 0.613400\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.0427  Validation Accuracy: 0.596200\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.0289  Validation Accuracy: 0.610800\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.0273  Validation Accuracy: 0.604200\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.0499  Validation Accuracy: 0.610200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.0307  Validation Accuracy: 0.610400\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.0387  Validation Accuracy: 0.595000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.0303  Validation Accuracy: 0.610800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     0.0294  Validation Accuracy: 0.602400\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.0389  Validation Accuracy: 0.605200\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.0316  Validation Accuracy: 0.613200\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.0348  Validation Accuracy: 0.590200\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.0348  Validation Accuracy: 0.599400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.0313  Validation Accuracy: 0.597600\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.0424  Validation Accuracy: 0.607800\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.0331  Validation Accuracy: 0.612600\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.0260  Validation Accuracy: 0.590000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.0363  Validation Accuracy: 0.611000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     0.0320  Validation Accuracy: 0.596000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.0362  Validation Accuracy: 0.600400\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.0266  Validation Accuracy: 0.610400\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.0465  Validation Accuracy: 0.600800\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.0226  Validation Accuracy: 0.603000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.0364  Validation Accuracy: 0.591400\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.0205  Validation Accuracy: 0.605000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.0264  Validation Accuracy: 0.604400\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.0547  Validation Accuracy: 0.598200\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     0.0424  Validation Accuracy: 0.599600\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.0298  Validation Accuracy: 0.591400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.0330  Validation Accuracy: 0.600200\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.0339  Validation Accuracy: 0.599600\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.0277  Validation Accuracy: 0.604200\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.0192  Validation Accuracy: 0.600800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.0474  Validation Accuracy: 0.595600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.0305  Validation Accuracy: 0.605000\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.0258  Validation Accuracy: 0.605800\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.0467  Validation Accuracy: 0.594600\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     0.0241  Validation Accuracy: 0.604200\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.0406  Validation Accuracy: 0.596200\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.0274  Validation Accuracy: 0.606600\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.0296  Validation Accuracy: 0.602200\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.0451  Validation Accuracy: 0.598200\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     0.0212  Validation Accuracy: 0.600000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.0479  Validation Accuracy: 0.589600\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.0500  Validation Accuracy: 0.596000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.0372  Validation Accuracy: 0.605800\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.0659  Validation Accuracy: 0.588200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     0.0534  Validation Accuracy: 0.587800\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.0310  Validation Accuracy: 0.601600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.0376  Validation Accuracy: 0.610000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.0338  Validation Accuracy: 0.605000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.0610  Validation Accuracy: 0.587800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.0355  Validation Accuracy: 0.585600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.0478  Validation Accuracy: 0.604200\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.0398  Validation Accuracy: 0.608400\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.0594  Validation Accuracy: 0.602000\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.0496  Validation Accuracy: 0.589600\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.0475  Validation Accuracy: 0.579800\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.1045  Validation Accuracy: 0.591800\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.0355  Validation Accuracy: 0.607000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.0591  Validation Accuracy: 0.602600\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.0598  Validation Accuracy: 0.585400\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.0421  Validation Accuracy: 0.588400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.0544  Validation Accuracy: 0.605800\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.0384  Validation Accuracy: 0.611000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.0677  Validation Accuracy: 0.604200\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.0570  Validation Accuracy: 0.570200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.0342  Validation Accuracy: 0.598200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcbfb01df98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

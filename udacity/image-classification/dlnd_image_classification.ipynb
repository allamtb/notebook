{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 2:\n",
      "Image - Min Value: 20 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCXoAKEzQA0Bhgh4AChP0AFDY/wb1gCgl/MrBSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a491b6240>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 2\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    \n",
    "    return  a +  (x - grayscale_min + b-1) / (grayscale_max - grayscale_min)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    onehots = []\n",
    "    for each in x :\n",
    "        zero= np.zeros(10)\n",
    "        zero[each] =1\n",
    "        onehots.append(zero)\n",
    "    return np.array(onehots)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return  tf.placeholder(tf.float32, shape=[None, image_shape[0],  image_shape[1],  image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,shape=[None,n_classes],name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 5)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # 卷积\n",
    "        #设置卷积核参数\n",
    "    print(x_tensor.get_shape())\n",
    "    FilterW= tf.Variable(tf.truncated_normal((conv_ksize[0], conv_ksize[1], x_tensor.get_shape()[3].value, conv_num_outputs),stddev=0.05)) ## (height, width, input_depth, output_depth)\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    bias = tf.random_normal([conv_num_outputs])\n",
    "    conv = tf.nn.conv2d(x_tensor, FilterW, strides, padding)\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv= tf.nn.relu(conv)\n",
    "    #池化\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    padding = 'SAME'\n",
    "    return tf.nn.max_pool(conv, ksize, strides, padding)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.reshape(x_tensor, [-1, x_tensor.shape[1].value * x_tensor.shape[2].value* x_tensor.shape[3].value])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.1))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fc1 =tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    return fc1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    weight= tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], mean=0.0, stddev=0.01))\n",
    "    #bias =  tf.Variable(tf.random_normal([num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n",
      "(?, 8, 8, 64)\n",
      "(?, 2, 2, 128)\n",
      "(?, 32, 32, 3)\n",
      "(?, 8, 8, 64)\n",
      "(?, 2, 2, 128)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 64, [3,3], [2,2], [2,2], [2,2])\n",
    "    conv2 = conv2d_maxpool(conv1, 128, [3,3], [2,2], [2,2], [2,2])\n",
    "    conv3 = conv2d_maxpool(conv2, 256, [3,3], [2,2], [2,2], [2,2])\n",
    "    \n",
    "    \n",
    "\n",
    "    # TODApply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fl0 = flatten(conv3)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    fl1 = fully_conn(fl0, 100)\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    fl1 = tf.nn.dropout(fl1, keep_prob)\n",
    "    \n",
    "    fl2 = fully_conn(fl1, 50)\n",
    "    fl2= tf.nn.relu(fl2)\n",
    "    fl2 = tf.nn.dropout(fl2, keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    out = output(fl2,10)\n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,feed_dict={x:feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y: label_batch, keep_prob:1.0 })\n",
    "    valid_acc =session.run(accuracy, feed_dict={x:valid_features, y: valid_labels, keep_prob:1.0})\n",
    "    print('Loss: {:>10.4f}  Validation Accuracy: {:.6f}'.format(loss, valid_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3042  Validation Accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3018  Validation Accuracy: 0.097000\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.3023  Validation Accuracy: 0.099800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.3028  Validation Accuracy: 0.097800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.3027  Validation Accuracy: 0.099800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.3028  Validation Accuracy: 0.099800\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.3020  Validation Accuracy: 0.105000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.3027  Validation Accuracy: 0.105000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.3021  Validation Accuracy: 0.105000\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.3005  Validation Accuracy: 0.102000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.3016  Validation Accuracy: 0.097000\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.2999  Validation Accuracy: 0.099800\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.3026  Validation Accuracy: 0.102600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     2.2339  Validation Accuracy: 0.149600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.2054  Validation Accuracy: 0.167200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.2351  Validation Accuracy: 0.197400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     2.2400  Validation Accuracy: 0.192400\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     2.2075  Validation Accuracy: 0.220000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     2.1808  Validation Accuracy: 0.121800\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     2.2183  Validation Accuracy: 0.207200\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     2.2104  Validation Accuracy: 0.175000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     2.1919  Validation Accuracy: 0.214200\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     2.2147  Validation Accuracy: 0.207600\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     2.2208  Validation Accuracy: 0.200400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     2.1888  Validation Accuracy: 0.217000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     2.2022  Validation Accuracy: 0.227400\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     2.2263  Validation Accuracy: 0.238000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     2.1928  Validation Accuracy: 0.205400\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     2.1601  Validation Accuracy: 0.210400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     2.2004  Validation Accuracy: 0.238600\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     2.2714  Validation Accuracy: 0.204800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     2.1988  Validation Accuracy: 0.233600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     2.2014  Validation Accuracy: 0.223800\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     2.1673  Validation Accuracy: 0.221600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     2.1729  Validation Accuracy: 0.250200\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     2.1669  Validation Accuracy: 0.200200\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     2.1839  Validation Accuracy: 0.228600\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     2.1444  Validation Accuracy: 0.251000\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     2.2064  Validation Accuracy: 0.255000\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     2.1677  Validation Accuracy: 0.216600\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     2.1312  Validation Accuracy: 0.229000\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     2.1182  Validation Accuracy: 0.261600\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     2.1326  Validation Accuracy: 0.257000\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     2.1621  Validation Accuracy: 0.264400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     2.1515  Validation Accuracy: 0.224200\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     2.0998  Validation Accuracy: 0.257800\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     2.2077  Validation Accuracy: 0.244600\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     2.1404  Validation Accuracy: 0.258400\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     2.1004  Validation Accuracy: 0.248400\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     2.0654  Validation Accuracy: 0.268600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     2.0202  Validation Accuracy: 0.243600\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     2.1345  Validation Accuracy: 0.278400\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     2.1487  Validation Accuracy: 0.282800\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     2.1121  Validation Accuracy: 0.278800\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     2.0466  Validation Accuracy: 0.258400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     2.0919  Validation Accuracy: 0.272400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     2.0744  Validation Accuracy: 0.257000\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     2.0306  Validation Accuracy: 0.279000\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     2.0196  Validation Accuracy: 0.207400\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     2.0513  Validation Accuracy: 0.265200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.9708  Validation Accuracy: 0.284000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     2.0295  Validation Accuracy: 0.269200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.9836  Validation Accuracy: 0.280400\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.9855  Validation Accuracy: 0.285600\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     2.0495  Validation Accuracy: 0.298600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     2.0353  Validation Accuracy: 0.265800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.9426  Validation Accuracy: 0.287800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.9593  Validation Accuracy: 0.286000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     2.0068  Validation Accuracy: 0.293600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.9457  Validation Accuracy: 0.314000\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.9772  Validation Accuracy: 0.307200\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.8468  Validation Accuracy: 0.278000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.8380  Validation Accuracy: 0.278400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.8980  Validation Accuracy: 0.278800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.8402  Validation Accuracy: 0.281000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.9349  Validation Accuracy: 0.326200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.9343  Validation Accuracy: 0.292600\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.9377  Validation Accuracy: 0.310400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.8590  Validation Accuracy: 0.306400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.8786  Validation Accuracy: 0.326000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.8874  Validation Accuracy: 0.328200\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.9118  Validation Accuracy: 0.327200\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.7680  Validation Accuracy: 0.336000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.8466  Validation Accuracy: 0.345200\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.8816  Validation Accuracy: 0.332800\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.8613  Validation Accuracy: 0.334600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.8030  Validation Accuracy: 0.347600\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.9676  Validation Accuracy: 0.343400\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.8917  Validation Accuracy: 0.352200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.8166  Validation Accuracy: 0.355800\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.7608  Validation Accuracy: 0.355800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.8024  Validation Accuracy: 0.369000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.7755  Validation Accuracy: 0.331800\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.7868  Validation Accuracy: 0.343800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.7314  Validation Accuracy: 0.359200\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.7353  Validation Accuracy: 0.373600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.7394  Validation Accuracy: 0.346200\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.7886  Validation Accuracy: 0.360400\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.6823  Validation Accuracy: 0.373600\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.9654  Validation Accuracy: 0.320400\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.7608  Validation Accuracy: 0.349000\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.8128  Validation Accuracy: 0.364000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.7763  Validation Accuracy: 0.369200\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.6941  Validation Accuracy: 0.366000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.6674  Validation Accuracy: 0.339400\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.6625  Validation Accuracy: 0.380800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.7207  Validation Accuracy: 0.328600\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.5718  Validation Accuracy: 0.385200\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.8846  Validation Accuracy: 0.361200\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.5626  Validation Accuracy: 0.390000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.5672  Validation Accuracy: 0.381000\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.6520  Validation Accuracy: 0.375000\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.6969  Validation Accuracy: 0.381000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.5801  Validation Accuracy: 0.337600\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.5167  Validation Accuracy: 0.386200\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.7551  Validation Accuracy: 0.356000\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.5285  Validation Accuracy: 0.367400\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.5812  Validation Accuracy: 0.378200\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.5698  Validation Accuracy: 0.376400\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.4591  Validation Accuracy: 0.380800\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.4682  Validation Accuracy: 0.378400\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.5275  Validation Accuracy: 0.391200\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.4793  Validation Accuracy: 0.371000\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.5017  Validation Accuracy: 0.356800\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     1.4447  Validation Accuracy: 0.401200\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     1.5906  Validation Accuracy: 0.373200\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     1.3928  Validation Accuracy: 0.394800\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     1.5497  Validation Accuracy: 0.259600\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     1.4892  Validation Accuracy: 0.402800\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     1.5336  Validation Accuracy: 0.369200\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     1.4008  Validation Accuracy: 0.388800\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.4279  Validation Accuracy: 0.375000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.3711  Validation Accuracy: 0.385800\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     1.3606  Validation Accuracy: 0.394800\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     1.3271  Validation Accuracy: 0.411000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     1.4342  Validation Accuracy: 0.394800\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     1.5752  Validation Accuracy: 0.386000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     1.3710  Validation Accuracy: 0.415000\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     1.4726  Validation Accuracy: 0.371800\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     1.4161  Validation Accuracy: 0.404600\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     1.3458  Validation Accuracy: 0.405800\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     1.4003  Validation Accuracy: 0.416600\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     1.3737  Validation Accuracy: 0.404200\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     1.2587  Validation Accuracy: 0.401800\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     1.2335  Validation Accuracy: 0.413800\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     1.2698  Validation Accuracy: 0.411800\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     1.3955  Validation Accuracy: 0.401600\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     1.3348  Validation Accuracy: 0.418000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     1.2660  Validation Accuracy: 0.375600\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     1.2843  Validation Accuracy: 0.420800\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     1.2589  Validation Accuracy: 0.405400\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     1.1701  Validation Accuracy: 0.409200\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     1.1674  Validation Accuracy: 0.405400\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     1.2035  Validation Accuracy: 0.351200\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     1.3862  Validation Accuracy: 0.408400\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     1.1915  Validation Accuracy: 0.417400\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     1.1574  Validation Accuracy: 0.410600\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     1.2508  Validation Accuracy: 0.419000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     1.1325  Validation Accuracy: 0.417000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     1.2920  Validation Accuracy: 0.419400\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     1.3420  Validation Accuracy: 0.391200\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     1.1424  Validation Accuracy: 0.432200\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     1.2365  Validation Accuracy: 0.420600\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     1.1096  Validation Accuracy: 0.418000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     1.1552  Validation Accuracy: 0.416000\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     1.1261  Validation Accuracy: 0.417400\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     1.0800  Validation Accuracy: 0.418800\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     1.2714  Validation Accuracy: 0.417000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     1.0474  Validation Accuracy: 0.426200\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     1.1215  Validation Accuracy: 0.421800\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     1.0917  Validation Accuracy: 0.414400\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     1.0239  Validation Accuracy: 0.397400\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.9863  Validation Accuracy: 0.428400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     1.1353  Validation Accuracy: 0.423200\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.9941  Validation Accuracy: 0.425800\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     1.1438  Validation Accuracy: 0.419800\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     1.1691  Validation Accuracy: 0.412800\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     1.0389  Validation Accuracy: 0.418400\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     1.0247  Validation Accuracy: 0.438600\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     1.0977  Validation Accuracy: 0.423400\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     1.0677  Validation Accuracy: 0.407800\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     1.0560  Validation Accuracy: 0.427400\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.9841  Validation Accuracy: 0.438600\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.9411  Validation Accuracy: 0.423400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     1.0321  Validation Accuracy: 0.436800\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.9918  Validation Accuracy: 0.401400\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.8744  Validation Accuracy: 0.424600\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     1.0091  Validation Accuracy: 0.423400\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.9989  Validation Accuracy: 0.423800\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.9564  Validation Accuracy: 0.425800\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.8887  Validation Accuracy: 0.408400\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.9964  Validation Accuracy: 0.435200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     1.0093  Validation Accuracy: 0.426400\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.9016  Validation Accuracy: 0.422600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.9553  Validation Accuracy: 0.434000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.8816  Validation Accuracy: 0.433800\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.8969  Validation Accuracy: 0.436800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.8893  Validation Accuracy: 0.450000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.8558  Validation Accuracy: 0.416400\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.9929  Validation Accuracy: 0.405400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3026  Validation Accuracy: 0.094000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3040  Validation Accuracy: 0.097000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2973  Validation Accuracy: 0.102000\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.3024  Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3001  Validation Accuracy: 0.094400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3029  Validation Accuracy: 0.097800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.3025  Validation Accuracy: 0.097800\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2949  Validation Accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.2958  Validation Accuracy: 0.098600\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.2974  Validation Accuracy: 0.134600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.2677  Validation Accuracy: 0.150000\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     2.1680  Validation Accuracy: 0.173800\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     2.1908  Validation Accuracy: 0.208000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     2.0921  Validation Accuracy: 0.214400\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     2.2877  Validation Accuracy: 0.165800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     2.2492  Validation Accuracy: 0.206000\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     2.1784  Validation Accuracy: 0.207800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     2.1203  Validation Accuracy: 0.181800\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     2.1280  Validation Accuracy: 0.196400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     2.2837  Validation Accuracy: 0.214400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     2.2903  Validation Accuracy: 0.228000\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     2.1736  Validation Accuracy: 0.213600\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     2.0260  Validation Accuracy: 0.229200\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     2.0850  Validation Accuracy: 0.224000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     2.2633  Validation Accuracy: 0.238800\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     2.2387  Validation Accuracy: 0.229200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     2.1641  Validation Accuracy: 0.239000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     2.0801  Validation Accuracy: 0.162200\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     2.0747  Validation Accuracy: 0.203200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     2.2449  Validation Accuracy: 0.215400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     2.2272  Validation Accuracy: 0.233400\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     2.1496  Validation Accuracy: 0.203800\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     2.0154  Validation Accuracy: 0.221200\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     2.0026  Validation Accuracy: 0.240000\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     2.2496  Validation Accuracy: 0.222000\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     2.1579  Validation Accuracy: 0.209200\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     2.0948  Validation Accuracy: 0.254200\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.9344  Validation Accuracy: 0.245600\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.9659  Validation Accuracy: 0.263200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     2.1701  Validation Accuracy: 0.219800\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     2.1964  Validation Accuracy: 0.260600\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     2.0741  Validation Accuracy: 0.247800\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.8455  Validation Accuracy: 0.251800\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     2.0609  Validation Accuracy: 0.264600\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     2.1596  Validation Accuracy: 0.238400\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     2.1385  Validation Accuracy: 0.283800\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     2.1766  Validation Accuracy: 0.262200\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.8545  Validation Accuracy: 0.276600\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.8674  Validation Accuracy: 0.244800\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     2.0263  Validation Accuracy: 0.270400\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     2.2264  Validation Accuracy: 0.271200\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     2.0755  Validation Accuracy: 0.293600\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.8412  Validation Accuracy: 0.277000\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.8459  Validation Accuracy: 0.281600\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     2.0360  Validation Accuracy: 0.271400\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     2.2731  Validation Accuracy: 0.295000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     2.0411  Validation Accuracy: 0.288400\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.7826  Validation Accuracy: 0.269400\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.8488  Validation Accuracy: 0.286600\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.9616  Validation Accuracy: 0.314000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     2.1712  Validation Accuracy: 0.255000\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.9446  Validation Accuracy: 0.285400\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.7704  Validation Accuracy: 0.335000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.7313  Validation Accuracy: 0.319400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     2.2540  Validation Accuracy: 0.302600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     2.1180  Validation Accuracy: 0.319200\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.9377  Validation Accuracy: 0.334200\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.8194  Validation Accuracy: 0.313800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.7483  Validation Accuracy: 0.342600\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.9265  Validation Accuracy: 0.285200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     2.0723  Validation Accuracy: 0.343200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.8953  Validation Accuracy: 0.326000\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.7592  Validation Accuracy: 0.293800\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.8575  Validation Accuracy: 0.318200\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.8813  Validation Accuracy: 0.321600\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     2.0914  Validation Accuracy: 0.305800\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.8502  Validation Accuracy: 0.342800\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.6580  Validation Accuracy: 0.326800\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.6984  Validation Accuracy: 0.341800\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.9365  Validation Accuracy: 0.351400\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     2.0493  Validation Accuracy: 0.311400\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.8759  Validation Accuracy: 0.367600\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.5505  Validation Accuracy: 0.348600\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.7641  Validation Accuracy: 0.363200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.9333  Validation Accuracy: 0.343200\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     2.0660  Validation Accuracy: 0.361200\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.9208  Validation Accuracy: 0.337200\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.6660  Validation Accuracy: 0.348400\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.6400  Validation Accuracy: 0.355000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.8749  Validation Accuracy: 0.361200\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.9758  Validation Accuracy: 0.354400\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.8355  Validation Accuracy: 0.380200\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.5731  Validation Accuracy: 0.349600\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.6834  Validation Accuracy: 0.357000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.8525  Validation Accuracy: 0.360200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     2.0948  Validation Accuracy: 0.379000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.7895  Validation Accuracy: 0.366600\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.6026  Validation Accuracy: 0.324400\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.5992  Validation Accuracy: 0.366000\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     2.0140  Validation Accuracy: 0.383400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.8913  Validation Accuracy: 0.378800\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.7545  Validation Accuracy: 0.348000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.5106  Validation Accuracy: 0.386200\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.5421  Validation Accuracy: 0.374800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.9128  Validation Accuracy: 0.361600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     2.0675  Validation Accuracy: 0.384000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.7564  Validation Accuracy: 0.362800\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.4827  Validation Accuracy: 0.363600\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.5349  Validation Accuracy: 0.364600\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.8661  Validation Accuracy: 0.389800\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.8385  Validation Accuracy: 0.356000\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.7391  Validation Accuracy: 0.388800\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.5314  Validation Accuracy: 0.362600\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.5170  Validation Accuracy: 0.392400\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.7071  Validation Accuracy: 0.391200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.8785  Validation Accuracy: 0.403200\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.7596  Validation Accuracy: 0.367000\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.4473  Validation Accuracy: 0.386600\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.4496  Validation Accuracy: 0.385600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.7522  Validation Accuracy: 0.393000\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.7918  Validation Accuracy: 0.413200\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.7006  Validation Accuracy: 0.392400\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.4146  Validation Accuracy: 0.402600\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.5724  Validation Accuracy: 0.366600\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.7571  Validation Accuracy: 0.401400\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.9695  Validation Accuracy: 0.353400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.7879  Validation Accuracy: 0.378400\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.3495  Validation Accuracy: 0.423400\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.4934  Validation Accuracy: 0.383400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.7082  Validation Accuracy: 0.381600\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.7840  Validation Accuracy: 0.382000\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.6704  Validation Accuracy: 0.405600\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.3271  Validation Accuracy: 0.397000\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.3619  Validation Accuracy: 0.364600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.7170  Validation Accuracy: 0.400600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.6952  Validation Accuracy: 0.343000\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.7890  Validation Accuracy: 0.394800\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.3296  Validation Accuracy: 0.422200\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.4817  Validation Accuracy: 0.410600\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.7806  Validation Accuracy: 0.407800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.8532  Validation Accuracy: 0.418200\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.7314  Validation Accuracy: 0.420400\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.4261  Validation Accuracy: 0.392600\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.4268  Validation Accuracy: 0.429600\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.7765  Validation Accuracy: 0.407800\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.6068  Validation Accuracy: 0.422600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.6518  Validation Accuracy: 0.427400\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.3101  Validation Accuracy: 0.423800\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.4342  Validation Accuracy: 0.417800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.7093  Validation Accuracy: 0.427400\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.7174  Validation Accuracy: 0.445400\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.5413  Validation Accuracy: 0.411400\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.2724  Validation Accuracy: 0.437200\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.2857  Validation Accuracy: 0.414000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.6374  Validation Accuracy: 0.421600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.8936  Validation Accuracy: 0.426800\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.5767  Validation Accuracy: 0.430200\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     1.3058  Validation Accuracy: 0.404800\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.3911  Validation Accuracy: 0.418400\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.5582  Validation Accuracy: 0.442800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.6564  Validation Accuracy: 0.436200\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.5742  Validation Accuracy: 0.426800\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     1.4349  Validation Accuracy: 0.427600\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.3160  Validation Accuracy: 0.428600\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.6237  Validation Accuracy: 0.395400\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.7534  Validation Accuracy: 0.411000\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.6375  Validation Accuracy: 0.399200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     1.2518  Validation Accuracy: 0.444800\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.3636  Validation Accuracy: 0.393200\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.5197  Validation Accuracy: 0.402600\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.6252  Validation Accuracy: 0.446400\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.5828  Validation Accuracy: 0.436000\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     1.3538  Validation Accuracy: 0.440800\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.3019  Validation Accuracy: 0.423600\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.6550  Validation Accuracy: 0.432800\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.6378  Validation Accuracy: 0.448600\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.4345  Validation Accuracy: 0.452800\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.2734  Validation Accuracy: 0.413800\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.3662  Validation Accuracy: 0.394400\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.4725  Validation Accuracy: 0.436800\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.5981  Validation Accuracy: 0.406600\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.4927  Validation Accuracy: 0.425400\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     1.2365  Validation Accuracy: 0.469200\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.3826  Validation Accuracy: 0.429600\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.5280  Validation Accuracy: 0.437200\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.5682  Validation Accuracy: 0.451800\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.4438  Validation Accuracy: 0.446400\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     1.2670  Validation Accuracy: 0.439200\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.2324  Validation Accuracy: 0.452800\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.5335  Validation Accuracy: 0.427600\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.4906  Validation Accuracy: 0.448800\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.5040  Validation Accuracy: 0.466200\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     1.2436  Validation Accuracy: 0.450400\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.2581  Validation Accuracy: 0.446200\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.4865  Validation Accuracy: 0.456600\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.4704  Validation Accuracy: 0.429400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     1.4385  Validation Accuracy: 0.442000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     1.1721  Validation Accuracy: 0.466200\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.2564  Validation Accuracy: 0.463800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.5388  Validation Accuracy: 0.454000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.5073  Validation Accuracy: 0.466600\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     1.4207  Validation Accuracy: 0.458800\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     1.1538  Validation Accuracy: 0.456600\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.2386  Validation Accuracy: 0.452600\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.5153  Validation Accuracy: 0.474600\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.5629  Validation Accuracy: 0.470200\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     1.5351  Validation Accuracy: 0.441200\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     1.1231  Validation Accuracy: 0.444000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.2645  Validation Accuracy: 0.466400\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     1.5576  Validation Accuracy: 0.457000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.4207  Validation Accuracy: 0.460800\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     1.5794  Validation Accuracy: 0.468000\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     1.1826  Validation Accuracy: 0.433200\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.3590  Validation Accuracy: 0.443800\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     1.3969  Validation Accuracy: 0.472800\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.4660  Validation Accuracy: 0.476400\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     1.3672  Validation Accuracy: 0.463600\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     1.1333  Validation Accuracy: 0.464400\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.2009  Validation Accuracy: 0.474200\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     1.4664  Validation Accuracy: 0.450000\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.4523  Validation Accuracy: 0.473800\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     1.4635  Validation Accuracy: 0.441200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     1.1810  Validation Accuracy: 0.446200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     1.1845  Validation Accuracy: 0.449400\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     1.3891  Validation Accuracy: 0.456000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.4661  Validation Accuracy: 0.471000\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     1.3737  Validation Accuracy: 0.484600\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     1.1903  Validation Accuracy: 0.469400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.1328  Validation Accuracy: 0.455000\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     1.4630  Validation Accuracy: 0.473600\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.3347  Validation Accuracy: 0.464800\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     1.3440  Validation Accuracy: 0.481600\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     1.1767  Validation Accuracy: 0.468800\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     1.1524  Validation Accuracy: 0.449400\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     1.3637  Validation Accuracy: 0.479000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.4116  Validation Accuracy: 0.469800\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     1.4135  Validation Accuracy: 0.447200\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     1.1298  Validation Accuracy: 0.435000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     1.1332  Validation Accuracy: 0.471800\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     1.5185  Validation Accuracy: 0.466200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.4167  Validation Accuracy: 0.488800\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     1.3475  Validation Accuracy: 0.468600\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     1.0583  Validation Accuracy: 0.470000\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.1255  Validation Accuracy: 0.447200\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     1.4406  Validation Accuracy: 0.474600\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.4549  Validation Accuracy: 0.495800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     1.3579  Validation Accuracy: 0.427600\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     1.0638  Validation Accuracy: 0.482600\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.1020  Validation Accuracy: 0.473200\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     1.2983  Validation Accuracy: 0.430000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.5158  Validation Accuracy: 0.480800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     1.3416  Validation Accuracy: 0.456600\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     1.0979  Validation Accuracy: 0.484200\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     1.1429  Validation Accuracy: 0.458800\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     1.3336  Validation Accuracy: 0.489400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.3954  Validation Accuracy: 0.488000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     1.3090  Validation Accuracy: 0.477800\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     1.0673  Validation Accuracy: 0.474200\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.0999  Validation Accuracy: 0.474200\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     1.3154  Validation Accuracy: 0.460800\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.4528  Validation Accuracy: 0.473600\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     1.2222  Validation Accuracy: 0.485800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     1.0450  Validation Accuracy: 0.452400\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     1.1191  Validation Accuracy: 0.487000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     1.2420  Validation Accuracy: 0.479600\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.3011  Validation Accuracy: 0.492200\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     1.2150  Validation Accuracy: 0.490000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     1.0606  Validation Accuracy: 0.492000\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     1.1260  Validation Accuracy: 0.486800\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     1.2556  Validation Accuracy: 0.473400\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.3293  Validation Accuracy: 0.449400\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     1.2350  Validation Accuracy: 0.483800\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     1.0781  Validation Accuracy: 0.489800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     1.0335  Validation Accuracy: 0.492600\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     1.2279  Validation Accuracy: 0.481000\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2813  Validation Accuracy: 0.497200\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     1.2890  Validation Accuracy: 0.501200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     1.0966  Validation Accuracy: 0.468800\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     1.0669  Validation Accuracy: 0.503400\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     1.2502  Validation Accuracy: 0.491600\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.2212  Validation Accuracy: 0.479800\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     1.1973  Validation Accuracy: 0.471200\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     1.0189  Validation Accuracy: 0.473200\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     1.0827  Validation Accuracy: 0.492200\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     1.3237  Validation Accuracy: 0.477200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.3685  Validation Accuracy: 0.499400\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     1.1967  Validation Accuracy: 0.497600\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     1.0667  Validation Accuracy: 0.476600\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     1.0473  Validation Accuracy: 0.497600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     1.1983  Validation Accuracy: 0.493200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2172  Validation Accuracy: 0.510200\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     1.3521  Validation Accuracy: 0.501000\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     1.0070  Validation Accuracy: 0.486600\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     1.0648  Validation Accuracy: 0.485400\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     1.2552  Validation Accuracy: 0.495000\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2376  Validation Accuracy: 0.503400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     1.2664  Validation Accuracy: 0.490000\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     1.0962  Validation Accuracy: 0.501600\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     0.9836  Validation Accuracy: 0.504400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     1.1617  Validation Accuracy: 0.481000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.1950  Validation Accuracy: 0.504200\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     1.2199  Validation Accuracy: 0.494400\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     1.0010  Validation Accuracy: 0.481800\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     1.0063  Validation Accuracy: 0.499400\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     1.2583  Validation Accuracy: 0.455600\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2864  Validation Accuracy: 0.516200\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     1.2380  Validation Accuracy: 0.509200\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     0.9232  Validation Accuracy: 0.492800\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     1.0037  Validation Accuracy: 0.492600\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     1.1439  Validation Accuracy: 0.503400\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.1602  Validation Accuracy: 0.509400\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     1.1301  Validation Accuracy: 0.511400\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.9333  Validation Accuracy: 0.470400\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     0.9513  Validation Accuracy: 0.500200\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     1.0845  Validation Accuracy: 0.500800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.1762  Validation Accuracy: 0.461800\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     1.2721  Validation Accuracy: 0.485400\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     1.1154  Validation Accuracy: 0.501200\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     0.9645  Validation Accuracy: 0.503800\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     1.2128  Validation Accuracy: 0.460200\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.1681  Validation Accuracy: 0.484600\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     1.3152  Validation Accuracy: 0.506200\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     0.9565  Validation Accuracy: 0.482400\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     0.9274  Validation Accuracy: 0.502800\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     1.2856  Validation Accuracy: 0.483800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.3389  Validation Accuracy: 0.500000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     1.1302  Validation Accuracy: 0.506400\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.9710  Validation Accuracy: 0.505000\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     0.9691  Validation Accuracy: 0.505200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     1.1744  Validation Accuracy: 0.511000\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.2116  Validation Accuracy: 0.477000\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     1.1138  Validation Accuracy: 0.486000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     0.9431  Validation Accuracy: 0.508000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     0.9302  Validation Accuracy: 0.513600\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     1.1570  Validation Accuracy: 0.500400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.1547  Validation Accuracy: 0.506000\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     1.1305  Validation Accuracy: 0.508400\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     1.0107  Validation Accuracy: 0.496400\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     0.8394  Validation Accuracy: 0.499600\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     1.0843  Validation Accuracy: 0.499600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.2173  Validation Accuracy: 0.502400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     1.0672  Validation Accuracy: 0.489400\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     1.0665  Validation Accuracy: 0.506800\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     0.9007  Validation Accuracy: 0.505400\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     1.2025  Validation Accuracy: 0.489000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.1228  Validation Accuracy: 0.507800\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     1.1466  Validation Accuracy: 0.516000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     0.9560  Validation Accuracy: 0.504200\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     0.9394  Validation Accuracy: 0.484800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     1.1662  Validation Accuracy: 0.496600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.1617  Validation Accuracy: 0.493400\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     1.1094  Validation Accuracy: 0.491800\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     0.9314  Validation Accuracy: 0.491200\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     0.9355  Validation Accuracy: 0.512400\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     1.1452  Validation Accuracy: 0.495000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.2316  Validation Accuracy: 0.517000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     1.0206  Validation Accuracy: 0.510600\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     0.9428  Validation Accuracy: 0.491200\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     0.9588  Validation Accuracy: 0.507000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     1.0083  Validation Accuracy: 0.517600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.2516  Validation Accuracy: 0.509400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     1.3504  Validation Accuracy: 0.495200\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     0.9526  Validation Accuracy: 0.512400\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     0.9062  Validation Accuracy: 0.519600\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     1.2314  Validation Accuracy: 0.508400\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.3257  Validation Accuracy: 0.504800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     1.0417  Validation Accuracy: 0.504600\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     0.8809  Validation Accuracy: 0.504000\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     0.9415  Validation Accuracy: 0.520800\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     1.0900  Validation Accuracy: 0.500800\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.3472  Validation Accuracy: 0.519000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     1.1088  Validation Accuracy: 0.499200\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     0.8425  Validation Accuracy: 0.521200\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     0.9165  Validation Accuracy: 0.505800\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     0.9945  Validation Accuracy: 0.509400\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.0072  Validation Accuracy: 0.510000\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     1.2699  Validation Accuracy: 0.499000\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     0.8436  Validation Accuracy: 0.503000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     0.8920  Validation Accuracy: 0.497400\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     1.0261  Validation Accuracy: 0.497600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.0675  Validation Accuracy: 0.519800\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     1.0832  Validation Accuracy: 0.513200\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     0.9441  Validation Accuracy: 0.501000\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     0.8446  Validation Accuracy: 0.497400\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     1.0549  Validation Accuracy: 0.497800\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.1767  Validation Accuracy: 0.529800\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     1.0100  Validation Accuracy: 0.516000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     0.8179  Validation Accuracy: 0.482200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     0.8528  Validation Accuracy: 0.515800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     1.0588  Validation Accuracy: 0.531400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.0846  Validation Accuracy: 0.521400\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     1.0001  Validation Accuracy: 0.497200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     0.8498  Validation Accuracy: 0.510800\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     0.8483  Validation Accuracy: 0.533200\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     0.9938  Validation Accuracy: 0.520400\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.0092  Validation Accuracy: 0.519200\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     1.0741  Validation Accuracy: 0.491600\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     0.8536  Validation Accuracy: 0.502800\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     0.8835  Validation Accuracy: 0.528200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     0.9603  Validation Accuracy: 0.509600\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.0320  Validation Accuracy: 0.504400\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     1.2117  Validation Accuracy: 0.515400\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     0.8157  Validation Accuracy: 0.514800\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     0.7990  Validation Accuracy: 0.528400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     1.0369  Validation Accuracy: 0.520400\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.0894  Validation Accuracy: 0.521200\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     0.9989  Validation Accuracy: 0.527600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     0.9080  Validation Accuracy: 0.510400\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     0.8659  Validation Accuracy: 0.529200\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     1.0951  Validation Accuracy: 0.514800\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.0181  Validation Accuracy: 0.521600\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     0.9608  Validation Accuracy: 0.520200\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     0.8384  Validation Accuracy: 0.520600\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     0.8078  Validation Accuracy: 0.511400\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     1.0514  Validation Accuracy: 0.520600\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.0596  Validation Accuracy: 0.525800\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     0.9159  Validation Accuracy: 0.524000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     0.8019  Validation Accuracy: 0.520400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     0.8443  Validation Accuracy: 0.522800\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     1.0037  Validation Accuracy: 0.517400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1673  Validation Accuracy: 0.519600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     0.9777  Validation Accuracy: 0.504400\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     0.8257  Validation Accuracy: 0.525600\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     0.8287  Validation Accuracy: 0.507800\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     0.9675  Validation Accuracy: 0.505000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     0.9500  Validation Accuracy: 0.510600\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     0.9414  Validation Accuracy: 0.483400\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     0.8083  Validation Accuracy: 0.526000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     0.7873  Validation Accuracy: 0.518000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     0.9947  Validation Accuracy: 0.523000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     0.9674  Validation Accuracy: 0.507800\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     1.0043  Validation Accuracy: 0.527800\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     0.7885  Validation Accuracy: 0.523600\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     0.7714  Validation Accuracy: 0.533400\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     0.9648  Validation Accuracy: 0.526400\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     0.9573  Validation Accuracy: 0.521400\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     0.9669  Validation Accuracy: 0.512200\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     0.8058  Validation Accuracy: 0.522000\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     0.7846  Validation Accuracy: 0.532800\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     0.9618  Validation Accuracy: 0.531800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.0301  Validation Accuracy: 0.535400\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     0.9518  Validation Accuracy: 0.496000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     0.7423  Validation Accuracy: 0.533600\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     0.8439  Validation Accuracy: 0.512600\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     0.9316  Validation Accuracy: 0.518400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     0.9795  Validation Accuracy: 0.512200\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     0.9048  Validation Accuracy: 0.521400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     0.8421  Validation Accuracy: 0.500000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     0.8012  Validation Accuracy: 0.510400\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     0.8755  Validation Accuracy: 0.520200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.0189  Validation Accuracy: 0.522800\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     0.9587  Validation Accuracy: 0.515000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     0.7959  Validation Accuracy: 0.521200\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     0.7941  Validation Accuracy: 0.510000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     0.8994  Validation Accuracy: 0.524200\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     0.9146  Validation Accuracy: 0.488000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     0.8722  Validation Accuracy: 0.516800\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.8581  Validation Accuracy: 0.525000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     0.7319  Validation Accuracy: 0.534400\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     1.1566  Validation Accuracy: 0.510600\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     0.8911  Validation Accuracy: 0.538400\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     0.9756  Validation Accuracy: 0.509200\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.7546  Validation Accuracy: 0.503000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     0.8278  Validation Accuracy: 0.515000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     0.9989  Validation Accuracy: 0.525600\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     0.9947  Validation Accuracy: 0.530600\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     0.8703  Validation Accuracy: 0.496200\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     0.6876  Validation Accuracy: 0.519400\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     0.8309  Validation Accuracy: 0.516000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     1.0956  Validation Accuracy: 0.505600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     0.9948  Validation Accuracy: 0.531000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     0.9048  Validation Accuracy: 0.528200\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     0.7542  Validation Accuracy: 0.507400\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     0.8755  Validation Accuracy: 0.534400\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     1.0215  Validation Accuracy: 0.531600\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     0.9778  Validation Accuracy: 0.520200\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     0.9209  Validation Accuracy: 0.525400\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     0.6467  Validation Accuracy: 0.521200\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     0.7992  Validation Accuracy: 0.505800\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     1.0196  Validation Accuracy: 0.508600\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     0.9375  Validation Accuracy: 0.529800\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     0.8848  Validation Accuracy: 0.530200\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.6931  Validation Accuracy: 0.512800\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     0.8334  Validation Accuracy: 0.516600\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     0.9310  Validation Accuracy: 0.522000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     0.8987  Validation Accuracy: 0.530600\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     0.8550  Validation Accuracy: 0.531000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     0.6779  Validation Accuracy: 0.510600\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     0.8224  Validation Accuracy: 0.514200\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     0.8749  Validation Accuracy: 0.533200\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     0.9278  Validation Accuracy: 0.530200\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     0.9196  Validation Accuracy: 0.525800\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.6643  Validation Accuracy: 0.499400\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     0.7489  Validation Accuracy: 0.542200\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     0.8235  Validation Accuracy: 0.528200\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     0.9038  Validation Accuracy: 0.522000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     0.9230  Validation Accuracy: 0.531400\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     0.6251  Validation Accuracy: 0.546000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     0.6831  Validation Accuracy: 0.530200\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     0.7646  Validation Accuracy: 0.534600\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.0169  Validation Accuracy: 0.535600\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     0.9007  Validation Accuracy: 0.520200\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.6846  Validation Accuracy: 0.519600\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     0.7483  Validation Accuracy: 0.524800\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     0.8568  Validation Accuracy: 0.512600\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     0.8800  Validation Accuracy: 0.530400\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     0.8322  Validation Accuracy: 0.519600\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.7883  Validation Accuracy: 0.524200\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     0.7938  Validation Accuracy: 0.536000\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     0.8291  Validation Accuracy: 0.496600\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     0.8550  Validation Accuracy: 0.529000\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     0.8335  Validation Accuracy: 0.538800\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     0.6562  Validation Accuracy: 0.531800\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     0.8466  Validation Accuracy: 0.535600\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     0.8299  Validation Accuracy: 0.533400\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     0.9476  Validation Accuracy: 0.524800\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     0.8520  Validation Accuracy: 0.535000\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     0.8070  Validation Accuracy: 0.496200\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     0.7888  Validation Accuracy: 0.522200\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     0.8165  Validation Accuracy: 0.533000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     0.9445  Validation Accuracy: 0.531200\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     0.8580  Validation Accuracy: 0.520200\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.7330  Validation Accuracy: 0.517000\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     0.7532  Validation Accuracy: 0.524200\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     0.8553  Validation Accuracy: 0.525000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     0.9131  Validation Accuracy: 0.528200\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     0.9955  Validation Accuracy: 0.530400\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.6503  Validation Accuracy: 0.531200\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     0.7084  Validation Accuracy: 0.529600\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     0.8192  Validation Accuracy: 0.539800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     0.8728  Validation Accuracy: 0.529400\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     0.8167  Validation Accuracy: 0.526600\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.6007  Validation Accuracy: 0.536200\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     0.7652  Validation Accuracy: 0.534600\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     0.8810  Validation Accuracy: 0.529200\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     0.9056  Validation Accuracy: 0.525600\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     0.8311  Validation Accuracy: 0.531200\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.6529  Validation Accuracy: 0.526600\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     0.7683  Validation Accuracy: 0.542400\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     0.8265  Validation Accuracy: 0.535000\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     0.8911  Validation Accuracy: 0.536200\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     0.7539  Validation Accuracy: 0.522400\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.6688  Validation Accuracy: 0.522200\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     0.7366  Validation Accuracy: 0.531800\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     0.8781  Validation Accuracy: 0.535800\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     0.8552  Validation Accuracy: 0.514000\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     0.7718  Validation Accuracy: 0.502400\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.5846  Validation Accuracy: 0.540800\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     0.6564  Validation Accuracy: 0.525000\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     1.1168  Validation Accuracy: 0.524000\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     0.8686  Validation Accuracy: 0.527800\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     0.8235  Validation Accuracy: 0.534000\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     0.5868  Validation Accuracy: 0.530400\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     0.7065  Validation Accuracy: 0.532400\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     0.9470  Validation Accuracy: 0.526400\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     0.7855  Validation Accuracy: 0.530000\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     0.8023  Validation Accuracy: 0.531800\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.6452  Validation Accuracy: 0.522000\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     0.7793  Validation Accuracy: 0.524600\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     0.8981  Validation Accuracy: 0.543800\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     0.9208  Validation Accuracy: 0.528400\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     0.7941  Validation Accuracy: 0.538600\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.6054  Validation Accuracy: 0.537800\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     0.7341  Validation Accuracy: 0.539000\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     0.8347  Validation Accuracy: 0.528200\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     0.8354  Validation Accuracy: 0.535800\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     0.7698  Validation Accuracy: 0.536000\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.5819  Validation Accuracy: 0.534200\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     0.7011  Validation Accuracy: 0.536000\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     0.6765  Validation Accuracy: 0.542400\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     0.8752  Validation Accuracy: 0.543000\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     0.7915  Validation Accuracy: 0.521800\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     0.7101  Validation Accuracy: 0.535000\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     0.6978  Validation Accuracy: 0.527200\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     0.8414  Validation Accuracy: 0.538000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     0.8091  Validation Accuracy: 0.541600\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     0.8492  Validation Accuracy: 0.510800\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     0.6253  Validation Accuracy: 0.541800\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     0.6945  Validation Accuracy: 0.526800\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     0.7187  Validation Accuracy: 0.537400\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     0.8323  Validation Accuracy: 0.536400\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     0.9403  Validation Accuracy: 0.535600\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.5696  Validation Accuracy: 0.539600\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     0.6225  Validation Accuracy: 0.542400\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     0.8929  Validation Accuracy: 0.530200\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     0.7578  Validation Accuracy: 0.535200\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     0.6867  Validation Accuracy: 0.537200\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     0.6129  Validation Accuracy: 0.537600\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     0.6546  Validation Accuracy: 0.542400\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     0.7373  Validation Accuracy: 0.534600\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.8265  Validation Accuracy: 0.534000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     0.8147  Validation Accuracy: 0.534000\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.5672  Validation Accuracy: 0.531800\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     0.7264  Validation Accuracy: 0.542600\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     0.7251  Validation Accuracy: 0.537400\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     0.7480  Validation Accuracy: 0.504200\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     0.7066  Validation Accuracy: 0.528800\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.5403  Validation Accuracy: 0.539000\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     0.7216  Validation Accuracy: 0.533800\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.7455  Validation Accuracy: 0.531600\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.7679  Validation Accuracy: 0.546000\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     0.7288  Validation Accuracy: 0.525800\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     0.5474  Validation Accuracy: 0.537600\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     0.7710  Validation Accuracy: 0.538000\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     0.7977  Validation Accuracy: 0.517600\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.8119  Validation Accuracy: 0.549400\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     0.7399  Validation Accuracy: 0.535800\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     0.5457  Validation Accuracy: 0.540600\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     0.6248  Validation Accuracy: 0.544800\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     0.7417  Validation Accuracy: 0.530600\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.7878  Validation Accuracy: 0.532200\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     0.7454  Validation Accuracy: 0.537600\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.6216  Validation Accuracy: 0.534600\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     0.7535  Validation Accuracy: 0.536600\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     0.6909  Validation Accuracy: 0.551800\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.7532  Validation Accuracy: 0.545400\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     0.7054  Validation Accuracy: 0.537000\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.7259  Validation Accuracy: 0.520800\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     0.6684  Validation Accuracy: 0.541800\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     0.8077  Validation Accuracy: 0.541600\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.8635  Validation Accuracy: 0.543600\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:     0.6862  Validation Accuracy: 0.541800\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:     0.6040  Validation Accuracy: 0.538800\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:     0.6229  Validation Accuracy: 0.532400\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:     0.7348  Validation Accuracy: 0.537600\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.7879  Validation Accuracy: 0.535800\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:     0.7368  Validation Accuracy: 0.529800\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:     0.5010  Validation Accuracy: 0.528200\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:     0.6243  Validation Accuracy: 0.538600\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:     0.7848  Validation Accuracy: 0.548000\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.8262  Validation Accuracy: 0.529800\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:     0.7732  Validation Accuracy: 0.546400\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:     0.5378  Validation Accuracy: 0.534000\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:     0.7580  Validation Accuracy: 0.542400\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:     0.7365  Validation Accuracy: 0.541200\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.8057  Validation Accuracy: 0.548000\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:     0.7015  Validation Accuracy: 0.540600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:     0.5553  Validation Accuracy: 0.537400\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:     0.6299  Validation Accuracy: 0.545200\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:     0.8740  Validation Accuracy: 0.541400\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.8555  Validation Accuracy: 0.536600\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:     0.7442  Validation Accuracy: 0.542600\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:     0.5651  Validation Accuracy: 0.538600\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:     0.7007  Validation Accuracy: 0.529000\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:     0.6634  Validation Accuracy: 0.547400\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.8111  Validation Accuracy: 0.518800\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:     0.7686  Validation Accuracy: 0.527200\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:     0.5294  Validation Accuracy: 0.541000\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:     0.6967  Validation Accuracy: 0.543400\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:     0.6658  Validation Accuracy: 0.529000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.7695  Validation Accuracy: 0.536800\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:     0.7420  Validation Accuracy: 0.550000\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:     0.4984  Validation Accuracy: 0.537600\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:     0.5862  Validation Accuracy: 0.526800\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:     0.8348  Validation Accuracy: 0.525000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     0.8045  Validation Accuracy: 0.541600\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:     0.8077  Validation Accuracy: 0.545000\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:     0.5198  Validation Accuracy: 0.530800\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:     0.6110  Validation Accuracy: 0.542000\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:     0.7479  Validation Accuracy: 0.535600\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     0.8338  Validation Accuracy: 0.538600\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:     0.7050  Validation Accuracy: 0.539800\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:     0.5051  Validation Accuracy: 0.533800\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:     0.5963  Validation Accuracy: 0.541800\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:     0.6294  Validation Accuracy: 0.535800\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.7998  Validation Accuracy: 0.537000\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:     0.7502  Validation Accuracy: 0.537000\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:     0.4902  Validation Accuracy: 0.546600\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:     0.6147  Validation Accuracy: 0.532600\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:     0.6324  Validation Accuracy: 0.551200\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.8881  Validation Accuracy: 0.526200\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:     0.6538  Validation Accuracy: 0.539000\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:     0.4796  Validation Accuracy: 0.539000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:     0.6596  Validation Accuracy: 0.543600\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:     0.6023  Validation Accuracy: 0.549000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.7612  Validation Accuracy: 0.550800\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:     0.6927  Validation Accuracy: 0.550400\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:     0.4771  Validation Accuracy: 0.529800\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:     0.6478  Validation Accuracy: 0.547400\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:     0.6830  Validation Accuracy: 0.524400\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.7433  Validation Accuracy: 0.549200\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:     0.7355  Validation Accuracy: 0.544400\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:     0.5160  Validation Accuracy: 0.538800\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:     0.6007  Validation Accuracy: 0.551200\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:     0.6686  Validation Accuracy: 0.538000\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.7609  Validation Accuracy: 0.550000\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:     0.7150  Validation Accuracy: 0.531600\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:     0.5300  Validation Accuracy: 0.529000\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:     0.5989  Validation Accuracy: 0.541800\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:     0.6383  Validation Accuracy: 0.538600\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.7274  Validation Accuracy: 0.549400\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:     0.6782  Validation Accuracy: 0.547600\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:     0.4595  Validation Accuracy: 0.532000\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:     0.6712  Validation Accuracy: 0.552800\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:     0.5977  Validation Accuracy: 0.539200\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.7791  Validation Accuracy: 0.536800\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:     0.6540  Validation Accuracy: 0.551600\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:     0.5012  Validation Accuracy: 0.529400\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:     0.6562  Validation Accuracy: 0.536400\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:     0.6030  Validation Accuracy: 0.542200\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.7440  Validation Accuracy: 0.549000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:     0.6444  Validation Accuracy: 0.525800\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:     0.4523  Validation Accuracy: 0.541400\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:     0.6598  Validation Accuracy: 0.532400\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:     0.5935  Validation Accuracy: 0.544400\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.7030  Validation Accuracy: 0.526200\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:     0.6396  Validation Accuracy: 0.543000\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:     0.4377  Validation Accuracy: 0.549200\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:     0.6923  Validation Accuracy: 0.523200\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:     0.6817  Validation Accuracy: 0.539000\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.8125  Validation Accuracy: 0.542800\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:     0.6061  Validation Accuracy: 0.544000\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:     0.5172  Validation Accuracy: 0.547000\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:     0.6383  Validation Accuracy: 0.550000\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:     0.5576  Validation Accuracy: 0.545000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.7957  Validation Accuracy: 0.540200\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:     0.6659  Validation Accuracy: 0.542800\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:     0.4344  Validation Accuracy: 0.556400\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:     0.6539  Validation Accuracy: 0.525000\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:     0.5831  Validation Accuracy: 0.535000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.7421  Validation Accuracy: 0.545200\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:     0.7170  Validation Accuracy: 0.539800\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:     0.5882  Validation Accuracy: 0.541400\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:     0.6337  Validation Accuracy: 0.540600\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:     0.5835  Validation Accuracy: 0.545600\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.8284  Validation Accuracy: 0.549600\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:     0.6205  Validation Accuracy: 0.553600\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:     0.5545  Validation Accuracy: 0.548000\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:     0.6054  Validation Accuracy: 0.539000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:     0.6526  Validation Accuracy: 0.542200\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.7746  Validation Accuracy: 0.548000\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:     0.6788  Validation Accuracy: 0.548800\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:     0.4905  Validation Accuracy: 0.534800\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:     0.5002  Validation Accuracy: 0.535400\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:     0.7281  Validation Accuracy: 0.536400\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.6929  Validation Accuracy: 0.553400\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:     0.7203  Validation Accuracy: 0.532400\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:     0.4280  Validation Accuracy: 0.540200\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:     0.5814  Validation Accuracy: 0.541800\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:     0.6318  Validation Accuracy: 0.550800\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.7075  Validation Accuracy: 0.547200\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:     0.6458  Validation Accuracy: 0.541400\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:     0.4834  Validation Accuracy: 0.538400\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:     0.6302  Validation Accuracy: 0.542000\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:     0.5583  Validation Accuracy: 0.528800\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.7723  Validation Accuracy: 0.531400\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:     0.6184  Validation Accuracy: 0.545800\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:     0.3912  Validation Accuracy: 0.547800\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:     0.5193  Validation Accuracy: 0.539600\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:     0.7239  Validation Accuracy: 0.534200\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.8522  Validation Accuracy: 0.550600\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:     0.6167  Validation Accuracy: 0.541200\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:     0.4473  Validation Accuracy: 0.545400\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:     0.5278  Validation Accuracy: 0.553400\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:     0.5813  Validation Accuracy: 0.533600\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.7520  Validation Accuracy: 0.542400\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:     0.7137  Validation Accuracy: 0.538000\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:     0.4328  Validation Accuracy: 0.536800\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:     0.5287  Validation Accuracy: 0.543200\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:     0.6138  Validation Accuracy: 0.538000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.7926  Validation Accuracy: 0.550400\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:     0.6077  Validation Accuracy: 0.538600\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:     0.4279  Validation Accuracy: 0.539200\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:     0.5272  Validation Accuracy: 0.550800\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:     0.5479  Validation Accuracy: 0.538000\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.8702  Validation Accuracy: 0.541000\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:     0.5960  Validation Accuracy: 0.558600\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:     0.4378  Validation Accuracy: 0.540400\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:     0.5703  Validation Accuracy: 0.544200\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:     0.5991  Validation Accuracy: 0.550400\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.7514  Validation Accuracy: 0.535600\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:     0.6160  Validation Accuracy: 0.530400\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:     0.4479  Validation Accuracy: 0.550200\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:     0.5619  Validation Accuracy: 0.545400\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:     0.5355  Validation Accuracy: 0.544400\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.8017  Validation Accuracy: 0.544800\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:     0.5832  Validation Accuracy: 0.550800\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:     0.3980  Validation Accuracy: 0.530400\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:     0.5613  Validation Accuracy: 0.544400\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:     0.6528  Validation Accuracy: 0.549400\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.7397  Validation Accuracy: 0.549600\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:     0.6125  Validation Accuracy: 0.551800\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:     0.3904  Validation Accuracy: 0.545400\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:     0.5478  Validation Accuracy: 0.540800\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:     0.5567  Validation Accuracy: 0.548200\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.7401  Validation Accuracy: 0.552000\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:     0.6625  Validation Accuracy: 0.551000\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:     0.4956  Validation Accuracy: 0.544800\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:     0.5643  Validation Accuracy: 0.548400\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:     0.5997  Validation Accuracy: 0.551000\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.7337  Validation Accuracy: 0.546200\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:     0.6030  Validation Accuracy: 0.549000\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:     0.6285  Validation Accuracy: 0.547400\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:     0.5896  Validation Accuracy: 0.552200\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:     0.5377  Validation Accuracy: 0.553000\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.7068  Validation Accuracy: 0.544200\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:     0.6186  Validation Accuracy: 0.542400\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:     0.3519  Validation Accuracy: 0.537200\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:     0.5287  Validation Accuracy: 0.539600\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:     0.5748  Validation Accuracy: 0.553000\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.9416  Validation Accuracy: 0.555000\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:     0.6875  Validation Accuracy: 0.553800\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:     0.5096  Validation Accuracy: 0.544000\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:     0.5156  Validation Accuracy: 0.546800\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:     0.5152  Validation Accuracy: 0.557200\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.8161  Validation Accuracy: 0.551600\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:     0.5761  Validation Accuracy: 0.535800\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:     0.3916  Validation Accuracy: 0.540200\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:     0.5257  Validation Accuracy: 0.539200\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:     0.5464  Validation Accuracy: 0.557600\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.6764  Validation Accuracy: 0.550400\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:     0.6818  Validation Accuracy: 0.554400\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:     0.4484  Validation Accuracy: 0.551000\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:     0.5792  Validation Accuracy: 0.537400\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:     0.6765  Validation Accuracy: 0.535800\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.7140  Validation Accuracy: 0.553600\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:     0.5775  Validation Accuracy: 0.553200\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:     0.4617  Validation Accuracy: 0.549600\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:     0.5039  Validation Accuracy: 0.547400\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:     0.5596  Validation Accuracy: 0.557000\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.6565  Validation Accuracy: 0.543400\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:     0.5744  Validation Accuracy: 0.552600\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:     0.3575  Validation Accuracy: 0.549600\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:     0.4696  Validation Accuracy: 0.553400\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:     0.5694  Validation Accuracy: 0.545600\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.6436  Validation Accuracy: 0.549000\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:     0.5800  Validation Accuracy: 0.556000\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:     0.4121  Validation Accuracy: 0.559200\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:     0.4915  Validation Accuracy: 0.548400\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:     0.4768  Validation Accuracy: 0.539800\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.6570  Validation Accuracy: 0.550200\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:     0.5402  Validation Accuracy: 0.551800\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:     0.3849  Validation Accuracy: 0.544800\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:     0.5006  Validation Accuracy: 0.540600\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:     0.4586  Validation Accuracy: 0.554000\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.8237  Validation Accuracy: 0.540200\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:     0.5084  Validation Accuracy: 0.552800\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:     0.3654  Validation Accuracy: 0.561000\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:     0.5042  Validation Accuracy: 0.546800\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:     0.5186  Validation Accuracy: 0.558000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.6882  Validation Accuracy: 0.544800\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:     0.5647  Validation Accuracy: 0.546400\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:     0.4410  Validation Accuracy: 0.542800\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:     0.4587  Validation Accuracy: 0.550800\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:     0.5044  Validation Accuracy: 0.548600\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.6303  Validation Accuracy: 0.547000\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:     0.6248  Validation Accuracy: 0.544200\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:     0.3988  Validation Accuracy: 0.546000\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:     0.5217  Validation Accuracy: 0.551600\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:     0.4644  Validation Accuracy: 0.552000\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.7124  Validation Accuracy: 0.542600\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:     0.5340  Validation Accuracy: 0.548200\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:     0.4642  Validation Accuracy: 0.545400\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:     0.5395  Validation Accuracy: 0.548000\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:     0.5103  Validation Accuracy: 0.554000\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.6648  Validation Accuracy: 0.537000\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:     0.5630  Validation Accuracy: 0.533600\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:     0.3701  Validation Accuracy: 0.560000\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:     0.5631  Validation Accuracy: 0.558200\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:     0.5745  Validation Accuracy: 0.540800\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.7497  Validation Accuracy: 0.555800\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:     0.5792  Validation Accuracy: 0.557000\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:     0.5039  Validation Accuracy: 0.556400\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:     0.4971  Validation Accuracy: 0.560400\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:     0.5378  Validation Accuracy: 0.553200\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.7180  Validation Accuracy: 0.555200\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:     0.6308  Validation Accuracy: 0.534400\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:     0.4421  Validation Accuracy: 0.546800\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:     0.5382  Validation Accuracy: 0.551600\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:     0.5250  Validation Accuracy: 0.549600\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.7226  Validation Accuracy: 0.553400\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:     0.4898  Validation Accuracy: 0.560600\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:     0.3725  Validation Accuracy: 0.549800\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:     0.5552  Validation Accuracy: 0.558800\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:     0.4767  Validation Accuracy: 0.545000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.6969  Validation Accuracy: 0.549000\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:     0.5450  Validation Accuracy: 0.549800\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:     0.3124  Validation Accuracy: 0.549600\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:     0.5153  Validation Accuracy: 0.530800\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:     0.4865  Validation Accuracy: 0.556600\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.6463  Validation Accuracy: 0.554600\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:     0.5412  Validation Accuracy: 0.541200\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:     0.3860  Validation Accuracy: 0.551200\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:     0.4596  Validation Accuracy: 0.545200\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:     0.5209  Validation Accuracy: 0.559200\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.6564  Validation Accuracy: 0.537000\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:     0.4647  Validation Accuracy: 0.558800\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:     0.3787  Validation Accuracy: 0.550600\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:     0.4527  Validation Accuracy: 0.545600\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:     0.5562  Validation Accuracy: 0.553400\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.5984  Validation Accuracy: 0.553400\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:     0.5564  Validation Accuracy: 0.553800\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:     0.3405  Validation Accuracy: 0.530600\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:     0.5225  Validation Accuracy: 0.540200\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:     0.4504  Validation Accuracy: 0.555800\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.6537  Validation Accuracy: 0.548800\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:     0.5909  Validation Accuracy: 0.557000\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:     0.3495  Validation Accuracy: 0.554200\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:     0.6047  Validation Accuracy: 0.562000\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:     0.4749  Validation Accuracy: 0.542600\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.6350  Validation Accuracy: 0.557200\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:     0.5461  Validation Accuracy: 0.557400\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:     0.4189  Validation Accuracy: 0.550200\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:     0.4515  Validation Accuracy: 0.552800\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:     0.5130  Validation Accuracy: 0.546800\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.6895  Validation Accuracy: 0.550800\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:     0.5120  Validation Accuracy: 0.554000\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:     0.3865  Validation Accuracy: 0.553800\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:     0.5510  Validation Accuracy: 0.541000\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:     0.5516  Validation Accuracy: 0.557200\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.6678  Validation Accuracy: 0.555600\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:     0.5971  Validation Accuracy: 0.544400\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:     0.4118  Validation Accuracy: 0.553800\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:     0.4547  Validation Accuracy: 0.553200\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:     0.6167  Validation Accuracy: 0.550000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.6466  Validation Accuracy: 0.531400\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:     0.5149  Validation Accuracy: 0.560800\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:     0.3566  Validation Accuracy: 0.535200\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:     0.4701  Validation Accuracy: 0.527400\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:     0.4711  Validation Accuracy: 0.547400\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.7098  Validation Accuracy: 0.554400\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:     0.4904  Validation Accuracy: 0.548400\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:     0.3344  Validation Accuracy: 0.551200\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:     0.4569  Validation Accuracy: 0.556400\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:     0.4369  Validation Accuracy: 0.553000\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.6478  Validation Accuracy: 0.557600\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:     0.5891  Validation Accuracy: 0.557000\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:     0.4141  Validation Accuracy: 0.558400\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:     0.4076  Validation Accuracy: 0.542200\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:     0.4703  Validation Accuracy: 0.555800\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.6456  Validation Accuracy: 0.547800\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:     0.5251  Validation Accuracy: 0.541800\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:     0.3816  Validation Accuracy: 0.549600\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:     0.5038  Validation Accuracy: 0.556600\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:     0.6344  Validation Accuracy: 0.540000\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.5864  Validation Accuracy: 0.558000\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:     0.5420  Validation Accuracy: 0.540000\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:     0.4982  Validation Accuracy: 0.549200\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:     0.4787  Validation Accuracy: 0.564000\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:     0.5285  Validation Accuracy: 0.550400\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.6182  Validation Accuracy: 0.565000\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:     0.5651  Validation Accuracy: 0.560200\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:     0.3880  Validation Accuracy: 0.546400\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:     0.4984  Validation Accuracy: 0.540200\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:     0.5355  Validation Accuracy: 0.550000\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.5646  Validation Accuracy: 0.561400\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:     0.6199  Validation Accuracy: 0.563400\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:     0.4104  Validation Accuracy: 0.554800\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:     0.4899  Validation Accuracy: 0.558000\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:     0.5257  Validation Accuracy: 0.554800\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.5863  Validation Accuracy: 0.545600\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:     0.5181  Validation Accuracy: 0.547000\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:     0.3156  Validation Accuracy: 0.562000\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:     0.4386  Validation Accuracy: 0.543000\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:     0.5052  Validation Accuracy: 0.554400\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.6850  Validation Accuracy: 0.550000\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:     0.5354  Validation Accuracy: 0.552400\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:     0.3689  Validation Accuracy: 0.548400\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:     0.4085  Validation Accuracy: 0.557400\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:     0.4245  Validation Accuracy: 0.538200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.6432  Validation Accuracy: 0.559800\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:     0.4853  Validation Accuracy: 0.558800\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:     0.3412  Validation Accuracy: 0.561600\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:     0.4808  Validation Accuracy: 0.554600\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:     0.5000  Validation Accuracy: 0.548800\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.5958  Validation Accuracy: 0.554400\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:     0.4884  Validation Accuracy: 0.550200\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:     0.3089  Validation Accuracy: 0.555400\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:     0.4290  Validation Accuracy: 0.557200\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:     0.5036  Validation Accuracy: 0.550400\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.6154  Validation Accuracy: 0.551000\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:     0.5699  Validation Accuracy: 0.544000\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:     0.3064  Validation Accuracy: 0.537200\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:     0.4434  Validation Accuracy: 0.554600\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:     0.4021  Validation Accuracy: 0.558200\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.5599  Validation Accuracy: 0.550600\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:     0.4225  Validation Accuracy: 0.553600\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:     0.4165  Validation Accuracy: 0.557600\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:     0.4354  Validation Accuracy: 0.547800\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:     0.4443  Validation Accuracy: 0.543600\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.6599  Validation Accuracy: 0.558200\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:     0.5083  Validation Accuracy: 0.553000\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:     0.3455  Validation Accuracy: 0.570000\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:     0.4139  Validation Accuracy: 0.554800\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:     0.4896  Validation Accuracy: 0.541800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.6218  Validation Accuracy: 0.532000\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:     0.5316  Validation Accuracy: 0.558600\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:     0.3598  Validation Accuracy: 0.538800\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:     0.3998  Validation Accuracy: 0.552800\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:     0.4733  Validation Accuracy: 0.552000\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.6188  Validation Accuracy: 0.567600\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:     0.4798  Validation Accuracy: 0.553200\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:     0.3226  Validation Accuracy: 0.556000\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:     0.4167  Validation Accuracy: 0.549400\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:     0.4650  Validation Accuracy: 0.562600\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.5973  Validation Accuracy: 0.557000\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:     0.4923  Validation Accuracy: 0.547400\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:     0.3641  Validation Accuracy: 0.519800\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:     0.3903  Validation Accuracy: 0.551600\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:     0.4235  Validation Accuracy: 0.550000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
